{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.tokenizer_exceptions import URL_PATTERN\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text before spacy\n",
    "def cleanText(text):\n",
    "    # get rid of new line terminator\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\r\\n\", \" \").replace(\"  \", \" \")\n",
    "    return text\n",
    "\n",
    "# deal with more symbols to seperate tokens\n",
    "def custom_tokenizer_modified(nlp):\n",
    "    # spacy defaults: when the standard behaviour is required, \n",
    "    # they need to be included when subclassing the tokenizer\n",
    "    extended_prefixes = tuple(list(nlp.Defaults.prefixes) + [\"-\"])\n",
    "    prefix_re = compile_prefix_regex(extended_prefixes)\n",
    "    extended_suffixes = tuple(list(nlp.Defaults.suffixes) + [\"-\"])\n",
    "    suffix_re = compile_suffix_regex(extended_suffixes)\n",
    "\n",
    "    # extending the default url regex\n",
    "    url = URL_PATTERN\n",
    "    url_re = re.compile(url)\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                     suffix_search=suffix_re.search,\n",
    "                     token_match=url_re.match\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test customized tokenization on Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default read text in spanish\n",
    "nlp = spacy.load('es_core_news_lg')\n",
    "# Tokenize text using custom tokenizer\n",
    "nlp.tokenizer = custom_tokenizer_modified(nlp)\n",
    "# Load the text\n",
    "text_Sample = open(\"data/Sample-text.txt\", encoding=\"utf-8\").read()\n",
    "# clean the text\n",
    "clean_text_Sample = cleanText(text_Sample)\n",
    "# Write into a sequence of Token objects\n",
    "doc_Sample = nlp(clean_text_Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Deal with Hashtag\n",
    "###############################################################\n",
    "# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\n",
    "matcher.add(\"HASHTAG\", None, [{\"ORTH\": \"#\"}, {\"IS_ALPHA\": True}])\n",
    "\n",
    "# Register token extension for hashtag\n",
    "Token.set_extension(\"is_hashtag\", default=False, force=True)\n",
    "\n",
    "# Fit in text in matcher\n",
    "matches = matcher(doc_Sample)\n",
    "\n",
    "# Find hashtag and merge, assign hashtag label\n",
    "hashtags = []\n",
    "for match_id, start, end in matches:\n",
    "    if doc_Sample.vocab.strings[match_id] == \"HASHTAG\":\n",
    "        hashtags.append(doc_Sample[start:end])\n",
    "with doc_Sample.retokenize() as retokenizer:\n",
    "    for span in hashtags:\n",
    "        retokenizer.merge(span)\n",
    "        for token in span:\n",
    "            token._.is_hashtag = True\n",
    "##############################################################\n",
    "\n",
    "# Assign tag to tokens\n",
    "for i, token in enumerate(doc_Sample):\n",
    "    if token._.is_hashtag:\n",
    "        token.tag_ = 'Hashtag'\n",
    "    if token.like_url:\n",
    "        token.tag_ = 'URL'\n",
    "    if token.like_email:\n",
    "        token.tag_ = 'Email'\n",
    "    if token.is_stop:\n",
    "        token.tag_ = 'Stop Word'\n",
    "    if token.like_num:\n",
    "        token.tag_ = 'Number'\n",
    "    if token.is_punct:\n",
    "        token.tag_ = 'Punctuation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the tokens to data frame\n",
    "df_Sample = pd.DataFrame()\n",
    "df_Sample['Token'] = [token.text for token in doc_Sample]\n",
    "df_Sample['POS'] = [token.pos_ for token in doc_Sample]\n",
    "df_Sample['NE'] = [token.ent_iob_ for token in doc_Sample]\n",
    "df_Sample['Lemma'] = [token.lemma_ for token in doc_Sample]\n",
    "df_Sample['Tag'] = [token.tag_ for token in doc_Sample]\n",
    "df_Sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample_GS = pd.read_csv('data/Sample-GS.csv', encoding='utf-8')\n",
    "print(Sample_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the  OpinionArticles\n",
    "outer_df_Sample = pd.merge(Sample_GS, df_Sample, on='Token', how='outer', indicator='Exist')\n",
    "diff_df_Sample = outer_df_Sample.loc[outer_df_Sample['Exist'] != 'both']\n",
    "print(\"Number of non-matching token for Sample is %s\" % len(diff_df_Sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test customized tokenization on OpinionArticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default read text in spanish\n",
    "nlp = spacy.load('es_core_news_lg')\n",
    "# Tokenize text using custom tokenizer\n",
    "nlp.tokenizer = custom_tokenizer_modified(nlp)\n",
    "# Load the text\n",
    "text_OpinionArticles = open(\"data/OpinionArticles-text.txt\", encoding=\"utf-8\").read()\n",
    "# clean the text\n",
    "clean_text_OpinionArticles = cleanText(text_OpinionArticles)\n",
    "# Write into a sequence of Token objects\n",
    "doc = nlp(clean_text_OpinionArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Deal with Hashtag\n",
    "###############################################################\n",
    "# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\n",
    "matcher.add(\"HASHTAG\", None, [{\"ORTH\": \"#\"}, {\"IS_ALPHA\": True}])\n",
    "\n",
    "# Register token extension for hashtag\n",
    "Token.set_extension(\"is_hashtag\", default=False, force=True)\n",
    "\n",
    "# Fit in text in matcher\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Find hashtag and merge, assign hashtag label\n",
    "hashtags = []\n",
    "for match_id, start, end in matches:\n",
    "    if doc.vocab.strings[match_id] == \"HASHTAG\":\n",
    "        hashtags.append(doc[start:end])\n",
    "with doc.retokenize() as retokenizer:\n",
    "    for span in hashtags:\n",
    "        retokenizer.merge(span)\n",
    "        for token in span:\n",
    "            token._.is_hashtag = True\n",
    "##############################################################\n",
    "\n",
    "# Assign tag to tokens\n",
    "for i, token in enumerate(doc):\n",
    "    if token._.is_hashtag:\n",
    "        token.tag_ = 'Hashtag'\n",
    "    if token.like_url:\n",
    "        token.tag_ = 'URL'\n",
    "    if token.like_email:\n",
    "        token.tag_ = 'Email'\n",
    "    if token.is_stop:\n",
    "        token.tag_ = 'Stop Word'\n",
    "    if token.like_num:\n",
    "        token.tag_ = 'Number'\n",
    "    if token.is_punct:\n",
    "        token.tag_ = 'Punctuation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>POS</th>\n",
       "      <th>NE</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ARTICLE0003,\"Qué</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>O</td>\n",
       "      <td>ARTICLE0003,\"Qué</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "      <td>AUX</td>\n",
       "      <td>O</td>\n",
       "      <td>ser</td>\n",
       "      <td>Stop Word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>el</td>\n",
       "      <td>DET</td>\n",
       "      <td>O</td>\n",
       "      <td>el</td>\n",
       "      <td>Stop Word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>O</td>\n",
       "      <td>\"</td>\n",
       "      <td>Punctuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>sharenting</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>O</td>\n",
       "      <td>sharenting</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Token    POS NE             Lemma          Tag\n",
       "0  ARTICLE0003,\"Qué   INTJ  O  ARTICLE0003,\"Qué        PROPN\n",
       "1                es    AUX  O               ser    Stop Word\n",
       "2                el    DET  O                el    Stop Word\n",
       "3                 \"  PUNCT  O                 \"  Punctuation\n",
       "4        sharenting   INTJ  O        sharenting        PROPN"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the tokens to data frame\n",
    "df = pd.DataFrame()\n",
    "df['Token'] = [token.text for token in doc]\n",
    "df['POS'] = [token.pos_ for token in doc]\n",
    "df['NE'] = [token.ent_iob_ for token in doc]\n",
    "df['Lemma'] = [token.lemma_ for token in doc]\n",
    "df['Tag'] = [token.tag_ for token in doc]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       New_Index        Token Language Anglicism Adapted\n",
      "0              1  ARTICLE0003      NaN     FALSE     NaN\n",
      "1              2            ,      NaN     FALSE     NaN\n",
      "2              3            \"      NaN     FALSE     NaN\n",
      "3              4          Qué      NaN     FALSE     NaN\n",
      "4              5           es      NaN     FALSE     NaN\n",
      "...          ...          ...      ...       ...     ...\n",
      "13326      13327     millones      NaN     FALSE     NaN\n",
      "13327      13328           de      NaN     FALSE     NaN\n",
      "13328      13329   visitantes      NaN     FALSE     NaN\n",
      "13329      13330       únicos      NaN     FALSE     NaN\n",
      "13330      13331            \"      NaN     FALSE     NaN\n",
      "\n",
      "[13331 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "GoldStandard_csv_OA = pd.read_csv('data/OpinionArticlesRetokenized-GS.csv', encoding='utf-8')\n",
    "print(GoldStandard_csv_OA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      text-token   token-char        Token Adapted EngCategory\n",
      "0            1-1         0-11  ARTICLE0003       _           _\n",
      "1            1-2        11-12            ,       _           _\n",
      "2            1-3        12-13            \"       _           _\n",
      "3            1-4        13-16          Qué       _           _\n",
      "4            1-5        17-19           es       _           _\n",
      "...          ...          ...          ...     ...         ...\n",
      "13320     511-11  69361-69369     millones       _           _\n",
      "13321     511-12  69370-69372           de       _           _\n",
      "13322     511-13  69373-69383   visitantes       _           _\n",
      "13323     511-14  69384-69390       únicos       _           _\n",
      "13324     511-15  69390-69391            \"       _           _\n",
      "\n",
      "[13325 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "WebAnno_csv_OA = pd.read_csv('data/OpinionArticles-WebAnno-GS.csv', encoding='utf-8')\n",
    "print(WebAnno_csv_OA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-matching token for OpinionArticles is 77\n",
      "         New_Index          Token Language Anglicism Adapted   POS   NE  \\\n",
      "0              1.0    ARTICLE0003      NaN     FALSE     NaN   NaN  NaN   \n",
      "2246082      705.0              %      NaN     FALSE     NaN   NaN  NaN   \n",
      "2246083      768.0              %      NaN     FALSE     NaN   NaN  NaN   \n",
      "2246084     9026.0              %      NaN     FALSE     NaN   NaN  NaN   \n",
      "2246085    12523.0              %      NaN     FALSE     NaN   NaN  NaN   \n",
      "...            ...            ...      ...       ...     ...   ...  ...   \n",
      "2273376        NaN        Educ.ar      NaN       NaN     NaN  INTJ    B   \n",
      "2273377        NaN  libre/abierto      NaN       NaN     NaN  INTJ    O   \n",
      "2273378        NaN            10%      NaN       NaN     NaN   SYM    O   \n",
      "2273379        NaN            25%      NaN       NaN     NaN   SYM    O   \n",
      "2273380        NaN          12,8%      NaN       NaN     NaN   SYM    O   \n",
      "\n",
      "                 Lemma                              Tag       Exist  \n",
      "0                  NaN                              NaN   left_only  \n",
      "2246082            NaN                              NaN   left_only  \n",
      "2246083            NaN                              NaN   left_only  \n",
      "2246084            NaN                              NaN   left_only  \n",
      "2246085            NaN                              NaN   left_only  \n",
      "...                ...                              ...         ...  \n",
      "2273376        Educ.ar                              URL  right_only  \n",
      "2273377  libre/abierto                            PROPN  right_only  \n",
      "2273378            10%               SYM__NumForm=Digit  right_only  \n",
      "2273379            25%  SYM__NumForm=Digit|NumType=Frac  right_only  \n",
      "2273380          12,8%  SYM__NumForm=Digit|NumType=Frac  right_only  \n",
      "\n",
      "[77 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# check the  OpinionArticles\n",
    "outer_df_OA = pd.merge(GoldStandard_csv_OA, df, on='Token', how='outer', indicator='Exist')\n",
    "diff_df_OA = outer_df_OA.loc[outer_df_OA['Exist'] != 'both']\n",
    "print(\"Number of non-matching token for OpinionArticles is %s\" % len(diff_df_OA))\n",
    "print(diff_df_OA)\n",
    "diff_df_OA.to_csv(r'data/diff-token-OA.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
